{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchconcetrationresnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shumshersubashgautam/2MobileNetV2-BreastCancer/blob/master/pytorchconcetrationresnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0gKRqmKWPuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "f8d8a6bc-2965-45dd-ac5b-3b7ffebf1b4e"
      },
      "source": [
        "!git clone https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Facial-Expression-Recognition.Pytorch'...\n",
            "remote: Enumerating objects: 1158, done.\u001b[K\n",
            "remote: Total 1158 (delta 0), reused 0 (delta 0), pack-reused 1158\u001b[K\n",
            "Receiving objects: 100% (1158/1158), 5.40 MiB | 8.41 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jKNdiSYfsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79a171a6-fef9-4b30-acda-791e867cbec2"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2vJebq1YnMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad7e2422-a118-430b-8583-895e3395f08c"
      },
      "source": [
        "cd content"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyaEwCffYoy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b396d8b-40e4-47f4-e3f9-0006ae7f0562"
      },
      "source": [
        "cd /content/Facial-Expression-Recognition.Pytorch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Facial-Expression-Recognition.Pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEbv4JoiYrbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5819a8f6-3f19-4965-b921-efa5e9a3c17d"
      },
      "source": [
        "!python mainpro_CK+.py --model VGG19 --bs 128 --lr 0.01 --fold 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 2.052 | Acc: 26.077% (230/882) 7/7 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 1.954 | Acc: 6.061% (6/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 6.061\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 1.506 | Acc: 42.517% (375/882) 7/7 \n",
            " [============================>.] | Loss: 2.068 | Acc: 6.061% (6/99) 20/20 \n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 1.204 | Acc: 52.041% (459/882) 7/7 \n",
            " [============================>.] | Loss: 2.390 | Acc: 6.061% (6/99) 20/20 \n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.980 | Acc: 60.998% (538/882) 7/7 \n",
            " [============================>.] | Loss: 2.795 | Acc: 6.061% (6/99) 20/20 \n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.750 | Acc: 69.615% (614/882) 7/7 \n",
            " [============================>.] | Loss: 3.201 | Acc: 6.061% (6/99) 20/20 \n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.635 | Acc: 75.737% (668/882) 7/7 \n",
            " [============================>.] | Loss: 1.740 | Acc: 39.394% (39/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 39.394\n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.560 | Acc: 76.984% (679/882) 7/7 \n",
            " [============================>.] | Loss: 0.749 | Acc: 78.788% (78/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 78.788\n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.550 | Acc: 78.005% (688/882) 7/7 \n",
            " [============================>.] | Loss: 0.738 | Acc: 67.677% (67/99) 20/20 \n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.444 | Acc: 84.354% (744/882) 7/7 \n",
            " [============================>.] | Loss: 1.302 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.387 | Acc: 86.508% (763/882) 7/7 \n",
            " [============================>.] | Loss: 0.664 | Acc: 77.778% (77/99) 20/20 \n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.313 | Acc: 89.796% (792/882) 7/7 \n",
            " [============================>.] | Loss: 0.875 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.294 | Acc: 89.116% (786/882) 7/7 \n",
            " [============================>.] | Loss: 0.892 | Acc: 70.707% (70/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.198 | Acc: 92.857% (819/882) 7/7 \n",
            " [============================>.] | Loss: 0.657 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.246 | Acc: 91.497% (807/882) 7/7 \n",
            " [============================>.] | Loss: 0.809 | Acc: 78.788% (78/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.256 | Acc: 91.497% (807/882) 7/7 \n",
            " [============================>.] | Loss: 1.226 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.178 | Acc: 94.331% (832/882) 7/7 \n",
            " [============================>.] | Loss: 0.819 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.126 | Acc: 96.032% (847/882) 7/7 \n",
            " [============================>.] | Loss: 0.502 | Acc: 80.808% (80/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 80.808\n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.129 | Acc: 95.918% (846/882) 7/7 \n",
            " [============================>.] | Loss: 0.519 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.131 | Acc: 96.145% (848/882) 7/7 \n",
            " [============================>.] | Loss: 0.574 | Acc: 76.768% (76/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.095 | Acc: 97.392% (859/882) 7/7 \n",
            " [============================>.] | Loss: 0.453 | Acc: 81.818% (81/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 81.818\n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [=========================>....] | Loss: 0.092 | Acc: 96.939% (855/882) 7/7 \n",
            " [============================>.] | Loss: 0.415 | Acc: 84.848% (84/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 84.848\n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [=========================>....] | Loss: 0.074 | Acc: 97.619% (861/882) 7/7 \n",
            " [============================>.] | Loss: 0.280 | Acc: 91.919% (91/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 91.919\n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [=========================>....] | Loss: 0.046 | Acc: 98.299% (867/882) 7/7 \n",
            " [============================>.] | Loss: 0.454 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [=========================>....] | Loss: 0.028 | Acc: 99.093% (874/882) 7/7 \n",
            " [============================>.] | Loss: 0.323 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [=========================>....] | Loss: 0.029 | Acc: 99.093% (874/882) 7/7 \n",
            " [============================>.] | Loss: 0.318 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [=========================>....] | Loss: 0.023 | Acc: 99.320% (876/882) 7/7 \n",
            " [============================>.] | Loss: 0.292 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [=========================>....] | Loss: 0.017 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.258 | Acc: 92.929% (92/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 92.929\n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [=========================>....] | Loss: 0.019 | Acc: 99.433% (877/882) 7/7 \n",
            " [============================>.] | Loss: 0.243 | Acc: 92.929% (92/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [=========================>....] | Loss: 0.017 | Acc: 99.320% (876/882) 7/7 \n",
            " [============================>.] | Loss: 0.243 | Acc: 92.929% (92/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [=========================>....] | Loss: 0.020 | Acc: 99.320% (876/882) 7/7 \n",
            " [============================>.] | Loss: 0.253 | Acc: 93.939% (93/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 93.939\n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.249 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [=========================>....] | Loss: 0.017 | Acc: 99.433% (877/882) 7/7 \n",
            " [============================>.] | Loss: 0.250 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.247 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [=========================>....] | Loss: 0.017 | Acc: 99.433% (877/882) 7/7 \n",
            " [============================>.] | Loss: 0.247 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.433% (877/882) 7/7 \n",
            " [============================>.] | Loss: 0.248 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [=========================>....] | Loss: 0.012 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.248 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [=========================>....] | Loss: 0.012 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.249 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [=========================>....] | Loss: 0.013 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.248 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [=========================>....] | Loss: 0.011 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.245 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.243 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [=========================>....] | Loss: 0.014 | Acc: 99.546% (878/882) 7/7 \n",
            " [============================>.] | Loss: 0.244 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [=========================>....] | Loss: 0.014 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.244 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [=========================>....] | Loss: 0.013 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.244 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.248 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [=========================>....] | Loss: 0.012 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.243 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.246 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.546% (878/882) 7/7 \n",
            " [============================>.] | Loss: 0.245 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [=========================>....] | Loss: 0.012 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.247 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.248 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [=========================>....] | Loss: 0.013 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.247 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [=========================>....] | Loss: 0.013 | Acc: 99.887% (881/882) 7/7 \n",
            " [============================>.] | Loss: 0.247 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [=========================>....] | Loss: 0.014 | Acc: 99.320% (876/882) 7/7 \n",
            " [============================>.] | Loss: 0.250 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [=========================>....] | Loss: 0.015 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.246 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [=========================>....] | Loss: 0.013 | Acc: 99.546% (878/882) 7/7 \n",
            " [============================>.] | Loss: 0.248 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [=========================>....] | Loss: 0.014 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.246 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [=========================>....] | Loss: 0.013 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.246 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [=========================>....] | Loss: 0.013 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.245 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [=========================>....] | Loss: 0.012 | Acc: 99.660% (879/882) 7/7 \n",
            " [============================>.] | Loss: 0.244 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [=========================>....] | Loss: 0.014 | Acc: 99.433% (877/882) 7/7 \n",
            " [============================>.] | Loss: 0.246 | Acc: 93.939% (93/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [=========================>....] | Loss: 0.011 | Acc: 99.773% (880/882) 7/7 \n",
            " [============================>.] | Loss: 0.245 | Acc: 93.939% (93/99) 20/20 \n",
            "best_Test_acc: 93.939\n",
            "best_Test_acc_epoch: 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fBTFuloZD7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b3e3569-2b3a-4a00-8be8-3f2e5d8850a4"
      },
      "source": [
        "!python k_fold_train.py"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.304 | Acc: 24.263% (214/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 5.245 | Acc: 27.273% (27/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 27.273\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.046 | Acc: 38.322% (338/882) 28/28 \n",
            " [============================>.] | Loss: 1.997 | Acc: 40.404% (40/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 40.404\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.955 | Acc: 36.168% (319/882) 28/28 \n",
            " [============================>.] | Loss: 2.028 | Acc: 28.283% (28/99) 20/20 \n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.824 | Acc: 40.816% (360/882) 28/28 \n",
            " [============================>.] | Loss: 1.754 | Acc: 33.333% (33/99) 20/20 \n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.708 | Acc: 40.136% (354/882) 28/28 \n",
            " [============================>.] | Loss: 1.839 | Acc: 27.273% (27/99) 20/20 \n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.665 | Acc: 41.837% (369/882) 28/28 \n",
            " [============================>.] | Loss: 1.793 | Acc: 39.394% (39/99) 20/20 \n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.548 | Acc: 43.991% (388/882) 28/28 \n",
            " [============================>.] | Loss: 2.085 | Acc: 32.323% (32/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.426 | Acc: 50.113% (442/882) 28/28 \n",
            " [============================>.] | Loss: 1.365 | Acc: 50.505% (50/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 50.505\n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.332 | Acc: 52.608% (464/882) 28/28 \n",
            " [============================>.] | Loss: 1.393 | Acc: 47.475% (47/99) 20/20 \n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.192 | Acc: 57.483% (507/882) 28/28 \n",
            " [============================>.] | Loss: 1.283 | Acc: 51.515% (51/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 51.515\n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.077 | Acc: 62.358% (550/882) 28/28 \n",
            " [============================>.] | Loss: 1.033 | Acc: 62.626% (62/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 62.626\n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.942 | Acc: 64.512% (569/882) 28/28 \n",
            " [============================>.] | Loss: 0.900 | Acc: 67.677% (67/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 67.677\n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.977 | Acc: 63.605% (561/882) 28/28 \n",
            " [============================>.] | Loss: 1.089 | Acc: 58.586% (58/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.807 | Acc: 69.501% (613/882) 28/28 \n",
            " [============================>.] | Loss: 0.690 | Acc: 71.717% (71/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 71.717\n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.914 | Acc: 67.687% (597/882) 28/28 \n",
            " [============================>.] | Loss: 1.232 | Acc: 58.586% (58/99) 20/20 \n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.776 | Acc: 69.388% (612/882) 28/28 \n",
            " [============================>.] | Loss: 1.017 | Acc: 53.535% (53/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.742 | Acc: 70.522% (622/882) 28/28 \n",
            " [============================>.] | Loss: 0.831 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.641 | Acc: 74.603% (658/882) 28/28 \n",
            " [============================>.] | Loss: 1.149 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.668 | Acc: 74.263% (655/882) 28/28 \n",
            " [============================>.] | Loss: 0.821 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.634 | Acc: 75.510% (666/882) 28/28 \n",
            " [============================>.] | Loss: 0.900 | Acc: 75.758% (75/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 75.758\n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.590 | Acc: 74.943% (661/882) 28/28 \n",
            " [============================>.] | Loss: 0.760 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 0.621 | Acc: 75.737% (668/882) 28/28 \n",
            " [============================>.] | Loss: 0.829 | Acc: 68.687% (68/99) 20/20 \n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.516 | Acc: 80.385% (709/882) 28/28 \n",
            " [============================>.] | Loss: 0.677 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.421 | Acc: 84.467% (745/882) 28/28 \n",
            " [============================>.] | Loss: 1.262 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.352 | Acc: 85.488% (754/882) 28/28 \n",
            " [============================>.] | Loss: 0.766 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.332 | Acc: 86.281% (761/882) 28/28 \n",
            " [============================>.] | Loss: 0.465 | Acc: 78.788% (78/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 78.788\n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.235 | Acc: 90.703% (800/882) 28/28 \n",
            " [============================>.] | Loss: 0.521 | Acc: 78.788% (78/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.218 | Acc: 91.497% (807/882) 28/28 \n",
            " [============================>.] | Loss: 0.496 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.201 | Acc: 93.537% (825/882) 28/28 \n",
            " [============================>.] | Loss: 0.471 | Acc: 78.788% (78/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.211 | Acc: 92.517% (816/882) 28/28 \n",
            " [============================>.] | Loss: 0.452 | Acc: 77.778% (77/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.176 | Acc: 93.537% (825/882) 28/28 \n",
            " [============================>.] | Loss: 0.390 | Acc: 83.838% (83/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 83.838\n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.150 | Acc: 94.785% (836/882) 28/28 \n",
            " [============================>.] | Loss: 0.404 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.178 | Acc: 93.764% (827/882) 28/28 \n",
            " [============================>.] | Loss: 0.396 | Acc: 78.788% (78/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.138 | Acc: 95.351% (841/882) 28/28 \n",
            " [============================>.] | Loss: 0.418 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.147 | Acc: 94.558% (834/882) 28/28 \n",
            " [============================>.] | Loss: 0.430 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.144 | Acc: 96.032% (847/882) 28/28 \n",
            " [============================>.] | Loss: 0.404 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.142 | Acc: 95.125% (839/882) 28/28 \n",
            " [============================>.] | Loss: 0.408 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.139 | Acc: 96.145% (848/882) 28/28 \n",
            " [============================>.] | Loss: 0.409 | Acc: 84.848% (84/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 84.848\n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.138 | Acc: 95.351% (841/882) 28/28 \n",
            " [============================>.] | Loss: 0.419 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.144 | Acc: 95.578% (843/882) 28/28 \n",
            " [============================>.] | Loss: 0.425 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.144 | Acc: 94.558% (834/882) 28/28 \n",
            " [============================>.] | Loss: 0.422 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.105 | Acc: 97.052% (856/882) 28/28 \n",
            " [============================>.] | Loss: 0.407 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.118 | Acc: 96.145% (848/882) 28/28 \n",
            " [============================>.] | Loss: 0.399 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.111 | Acc: 97.166% (857/882) 28/28 \n",
            " [============================>.] | Loss: 0.401 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.123 | Acc: 96.712% (853/882) 28/28 \n",
            " [============================>.] | Loss: 0.408 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.115 | Acc: 96.599% (852/882) 28/28 \n",
            " [============================>.] | Loss: 0.421 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.127 | Acc: 96.032% (847/882) 28/28 \n",
            " [============================>.] | Loss: 0.404 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.136 | Acc: 96.145% (848/882) 28/28 \n",
            " [============================>.] | Loss: 0.397 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.115 | Acc: 97.166% (857/882) 28/28 \n",
            " [============================>.] | Loss: 0.406 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.155 | Acc: 94.785% (836/882) 28/28 \n",
            " [============================>.] | Loss: 0.409 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.141 | Acc: 95.351% (841/882) 28/28 \n",
            " [============================>.] | Loss: 0.417 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.098 | Acc: 97.506% (860/882) 28/28 \n",
            " [============================>.] | Loss: 0.391 | Acc: 85.859% (85/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 85.859\n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.109 | Acc: 96.599% (852/882) 28/28 \n",
            " [============================>.] | Loss: 0.386 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.133 | Acc: 95.465% (842/882) 28/28 \n",
            " [============================>.] | Loss: 0.400 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.123 | Acc: 95.465% (842/882) 28/28 \n",
            " [============================>.] | Loss: 0.411 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.112 | Acc: 96.825% (854/882) 28/28 \n",
            " [============================>.] | Loss: 0.397 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.136 | Acc: 95.918% (846/882) 28/28 \n",
            " [============================>.] | Loss: 0.384 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.118 | Acc: 96.145% (848/882) 28/28 \n",
            " [============================>.] | Loss: 0.394 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.128 | Acc: 96.032% (847/882) 28/28 \n",
            " [============================>.] | Loss: 0.423 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.141 | Acc: 96.032% (847/882) 28/28 \n",
            " [============================>.] | Loss: 0.407 | Acc: 83.838% (83/99) 20/20 \n",
            "best_Test_acc: 85.859\n",
            "best_Test_acc_epoch: 51\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.062 | Acc: 28.798% (254/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 2.080 | Acc: 24.242% (24/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 24.242\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.440 | Acc: 43.197% (381/882) 28/28 \n",
            " [============================>.] | Loss: 1.398 | Acc: 42.424% (42/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 42.424\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.349 | Acc: 43.764% (386/882) 28/28 \n",
            " [============================>.] | Loss: 1.364 | Acc: 42.424% (42/99) 20/20 \n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.502 | Acc: 43.878% (387/882) 28/28 \n",
            " [============================>.] | Loss: 1.361 | Acc: 45.455% (45/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 45.455\n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.334 | Acc: 45.238% (399/882) 28/28 \n",
            " [============================>.] | Loss: 1.430 | Acc: 52.525% (52/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 52.525\n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.337 | Acc: 45.351% (400/882) 28/28 \n",
            " [============================>.] | Loss: 1.875 | Acc: 53.535% (53/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 53.535\n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.261 | Acc: 48.980% (432/882) 28/28 \n",
            " [============================>.] | Loss: 1.278 | Acc: 47.475% (47/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.208 | Acc: 51.020% (450/882) 28/28 \n",
            " [============================>.] | Loss: 1.272 | Acc: 45.455% (45/99) 20/20 \n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.275 | Acc: 51.474% (454/882) 28/28 \n",
            " [============================>.] | Loss: 1.292 | Acc: 54.545% (54/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 54.545\n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.172 | Acc: 55.102% (486/882) 28/28 \n",
            " [============================>.] | Loss: 1.333 | Acc: 36.364% (36/99) 20/20 \n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.065 | Acc: 58.844% (519/882) 28/28 \n",
            " [============================>.] | Loss: 1.172 | Acc: 52.525% (52/99) 20/20 \n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.915 | Acc: 61.791% (545/882) 28/28 \n",
            " [============================>.] | Loss: 0.840 | Acc: 63.636% (63/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 63.636\n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.878 | Acc: 64.286% (567/882) 28/28 \n",
            " [============================>.] | Loss: 1.052 | Acc: 60.606% (60/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.891 | Acc: 62.472% (551/882) 28/28 \n",
            " [============================>.] | Loss: 0.929 | Acc: 61.616% (61/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.832 | Acc: 66.780% (589/882) 28/28 \n",
            " [============================>.] | Loss: 0.801 | Acc: 67.677% (67/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 67.677\n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.768 | Acc: 68.707% (606/882) 28/28 \n",
            " [============================>.] | Loss: 0.885 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.684 | Acc: 72.336% (638/882) 28/28 \n",
            " [============================>.] | Loss: 0.684 | Acc: 72.727% (72/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 72.727\n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.699 | Acc: 70.635% (623/882) 28/28 \n",
            " [============================>.] | Loss: 1.315 | Acc: 68.687% (68/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.708 | Acc: 72.336% (638/882) 28/28 \n",
            " [============================>.] | Loss: 0.946 | Acc: 75.758% (75/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 75.758\n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.493 | Acc: 79.932% (705/882) 28/28 \n",
            " [============================>.] | Loss: 0.944 | Acc: 59.596% (59/99) 20/20 \n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.505 | Acc: 79.365% (700/882) 28/28 \n",
            " [============================>.] | Loss: 0.497 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 0.411 | Acc: 83.560% (737/882) 28/28 \n",
            " [============================>.] | Loss: 0.591 | Acc: 83.838% (83/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 83.838\n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.305 | Acc: 86.961% (767/882) 28/28 \n",
            " [============================>.] | Loss: 0.454 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.312 | Acc: 88.095% (777/882) 28/28 \n",
            " [============================>.] | Loss: 0.705 | Acc: 78.788% (78/99) 20/20 \n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.266 | Acc: 89.342% (788/882) 28/28 \n",
            " [============================>.] | Loss: 0.527 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.221 | Acc: 91.610% (808/882) 28/28 \n",
            " [============================>.] | Loss: 0.629 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.156 | Acc: 93.991% (829/882) 28/28 \n",
            " [============================>.] | Loss: 0.533 | Acc: 85.859% (85/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 85.859\n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.142 | Acc: 94.898% (837/882) 28/28 \n",
            " [============================>.] | Loss: 0.576 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.115 | Acc: 96.032% (847/882) 28/28 \n",
            " [============================>.] | Loss: 0.507 | Acc: 86.869% (86/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 86.869\n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.111 | Acc: 96.939% (855/882) 28/28 \n",
            " [============================>.] | Loss: 0.452 | Acc: 88.889% (88/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 88.889\n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.093 | Acc: 97.052% (856/882) 28/28 \n",
            " [============================>.] | Loss: 0.576 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.094 | Acc: 97.392% (859/882) 28/28 \n",
            " [============================>.] | Loss: 0.531 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.084 | Acc: 97.732% (862/882) 28/28 \n",
            " [============================>.] | Loss: 0.503 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.081 | Acc: 98.186% (866/882) 28/28 \n",
            " [============================>.] | Loss: 0.466 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.061 | Acc: 98.639% (870/882) 28/28 \n",
            " [============================>.] | Loss: 0.538 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.065 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.477 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.066 | Acc: 98.186% (866/882) 28/28 \n",
            " [============================>.] | Loss: 0.519 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.062 | Acc: 98.186% (866/882) 28/28 \n",
            " [============================>.] | Loss: 0.503 | Acc: 89.899% (89/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 89.899\n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.072 | Acc: 97.732% (862/882) 28/28 \n",
            " [============================>.] | Loss: 0.471 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.063 | Acc: 98.413% (868/882) 28/28 \n",
            " [============================>.] | Loss: 0.499 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.070 | Acc: 98.073% (865/882) 28/28 \n",
            " [============================>.] | Loss: 0.444 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.057 | Acc: 98.413% (868/882) 28/28 \n",
            " [============================>.] | Loss: 0.451 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.059 | Acc: 98.866% (872/882) 28/28 \n",
            " [============================>.] | Loss: 0.455 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.058 | Acc: 98.980% (873/882) 28/28 \n",
            " [============================>.] | Loss: 0.450 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.062 | Acc: 98.639% (870/882) 28/28 \n",
            " [============================>.] | Loss: 0.452 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.058 | Acc: 98.980% (873/882) 28/28 \n",
            " [============================>.] | Loss: 0.501 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.062 | Acc: 99.206% (875/882) 28/28 \n",
            " [============================>.] | Loss: 0.483 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.061 | Acc: 98.866% (872/882) 28/28 \n",
            " [============================>.] | Loss: 0.463 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.077 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.492 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.049 | Acc: 99.206% (875/882) 28/28 \n",
            " [============================>.] | Loss: 0.421 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.057 | Acc: 98.753% (871/882) 28/28 \n",
            " [============================>.] | Loss: 0.465 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.067 | Acc: 98.639% (870/882) 28/28 \n",
            " [============================>.] | Loss: 0.478 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.070 | Acc: 98.073% (865/882) 28/28 \n",
            " [============================>.] | Loss: 0.475 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.061 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.490 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.061 | Acc: 98.413% (868/882) 28/28 \n",
            " [============================>.] | Loss: 0.487 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.065 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.495 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.060 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.453 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.055 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.496 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.066 | Acc: 98.186% (866/882) 28/28 \n",
            " [============================>.] | Loss: 0.486 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.062 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.462 | Acc: 89.899% (89/99) 20/20 \n",
            "best_Test_acc: 89.899\n",
            "best_Test_acc_epoch: 37\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.341 | Acc: 20.068% (177/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 46.418 | Acc: 9.091% (9/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 9.091\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.357 | Acc: 20.408% (180/882) 28/28 \n",
            " [============================>.] | Loss: 6.455 | Acc: 18.182% (18/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 18.182\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.377 | Acc: 22.109% (195/882) 28/28 \n",
            " [============================>.] | Loss: 8.449 | Acc: 24.242% (24/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 24.242\n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.521 | Acc: 20.181% (178/882) 28/28 \n",
            " [============================>.] | Loss: 1.886 | Acc: 18.182% (18/99) 20/20 \n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.207 | Acc: 21.429% (189/882) 28/28 \n",
            " [============================>.] | Loss: 1.878 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.101 | Acc: 25.170% (222/882) 28/28 \n",
            " [============================>.] | Loss: 1.884 | Acc: 21.212% (21/99) 20/20 \n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.948 | Acc: 27.551% (243/882) 28/28 \n",
            " [============================>.] | Loss: 1.901 | Acc: 21.212% (21/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.762 | Acc: 30.612% (270/882) 28/28 \n",
            " [============================>.] | Loss: 1.650 | Acc: 44.444% (44/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 44.444\n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.708 | Acc: 32.653% (288/882) 28/28 \n",
            " [============================>.] | Loss: 1.882 | Acc: 18.182% (18/99) 20/20 \n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.658 | Acc: 32.200% (284/882) 28/28 \n",
            " [============================>.] | Loss: 1.515 | Acc: 35.354% (35/99) 20/20 \n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.725 | Acc: 31.406% (277/882) 28/28 \n",
            " [============================>.] | Loss: 1.911 | Acc: 18.182% (18/99) 20/20 \n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.640 | Acc: 35.034% (309/882) 28/28 \n",
            " [============================>.] | Loss: 1.403 | Acc: 39.394% (39/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.577 | Acc: 36.848% (325/882) 28/28 \n",
            " [============================>.] | Loss: 1.646 | Acc: 35.354% (35/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.541 | Acc: 38.662% (341/882) 28/28 \n",
            " [============================>.] | Loss: 1.426 | Acc: 42.424% (42/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.410 | Acc: 42.857% (378/882) 28/28 \n",
            " [============================>.] | Loss: 1.456 | Acc: 48.485% (48/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 48.485\n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.361 | Acc: 46.372% (409/882) 28/28 \n",
            " [============================>.] | Loss: 1.369 | Acc: 41.414% (41/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.377 | Acc: 43.311% (382/882) 28/28 \n",
            " [============================>.] | Loss: 1.570 | Acc: 37.374% (37/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.343 | Acc: 46.825% (413/882) 28/28 \n",
            " [============================>.] | Loss: 1.506 | Acc: 41.414% (41/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.315 | Acc: 50.340% (444/882) 28/28 \n",
            " [============================>.] | Loss: 1.531 | Acc: 44.444% (44/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.175 | Acc: 55.329% (488/882) 28/28 \n",
            " [============================>.] | Loss: 1.079 | Acc: 59.596% (59/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 59.596\n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.127 | Acc: 57.483% (507/882) 28/28 \n",
            " [============================>.] | Loss: 1.169 | Acc: 59.596% (59/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 1.000 | Acc: 62.018% (547/882) 28/28 \n",
            " [============================>.] | Loss: 1.023 | Acc: 65.657% (65/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 65.657\n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.921 | Acc: 64.626% (570/882) 28/28 \n",
            " [============================>.] | Loss: 1.041 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.873 | Acc: 66.327% (585/882) 28/28 \n",
            " [============================>.] | Loss: 1.222 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.884 | Acc: 67.120% (592/882) 28/28 \n",
            " [============================>.] | Loss: 1.018 | Acc: 59.596% (59/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.817 | Acc: 69.728% (615/882) 28/28 \n",
            " [============================>.] | Loss: 0.807 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.807 | Acc: 69.841% (616/882) 28/28 \n",
            " [============================>.] | Loss: 0.880 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.724 | Acc: 74.150% (654/882) 28/28 \n",
            " [============================>.] | Loss: 0.909 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.716 | Acc: 74.490% (657/882) 28/28 \n",
            " [============================>.] | Loss: 0.850 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.680 | Acc: 75.397% (665/882) 28/28 \n",
            " [============================>.] | Loss: 0.853 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.630 | Acc: 77.438% (683/882) 28/28 \n",
            " [============================>.] | Loss: 0.777 | Acc: 66.667% (66/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 66.667\n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.636 | Acc: 76.417% (674/882) 28/28 \n",
            " [============================>.] | Loss: 0.741 | Acc: 71.717% (71/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 71.717\n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.616 | Acc: 76.757% (677/882) 28/28 \n",
            " [============================>.] | Loss: 0.721 | Acc: 72.727% (72/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 72.727\n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.592 | Acc: 78.345% (691/882) 28/28 \n",
            " [============================>.] | Loss: 0.805 | Acc: 67.677% (67/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.563 | Acc: 79.365% (700/882) 28/28 \n",
            " [============================>.] | Loss: 0.684 | Acc: 75.758% (75/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 75.758\n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.579 | Acc: 78.685% (694/882) 28/28 \n",
            " [============================>.] | Loss: 0.742 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.572 | Acc: 79.365% (700/882) 28/28 \n",
            " [============================>.] | Loss: 0.793 | Acc: 68.687% (68/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.557 | Acc: 79.365% (700/882) 28/28 \n",
            " [============================>.] | Loss: 0.699 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.540 | Acc: 79.478% (701/882) 28/28 \n",
            " [============================>.] | Loss: 0.712 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.572 | Acc: 79.365% (700/882) 28/28 \n",
            " [============================>.] | Loss: 0.738 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.541 | Acc: 79.819% (704/882) 28/28 \n",
            " [============================>.] | Loss: 0.731 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.570 | Acc: 79.365% (700/882) 28/28 \n",
            " [============================>.] | Loss: 0.768 | Acc: 70.707% (70/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.522 | Acc: 82.200% (725/882) 28/28 \n",
            " [============================>.] | Loss: 0.718 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.529 | Acc: 79.705% (703/882) 28/28 \n",
            " [============================>.] | Loss: 0.731 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.516 | Acc: 80.045% (706/882) 28/28 \n",
            " [============================>.] | Loss: 0.713 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.526 | Acc: 81.293% (717/882) 28/28 \n",
            " [============================>.] | Loss: 0.719 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.550 | Acc: 80.839% (713/882) 28/28 \n",
            " [============================>.] | Loss: 0.753 | Acc: 70.707% (70/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.506 | Acc: 81.406% (718/882) 28/28 \n",
            " [============================>.] | Loss: 0.734 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.532 | Acc: 81.746% (721/882) 28/28 \n",
            " [============================>.] | Loss: 0.732 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.531 | Acc: 81.066% (715/882) 28/28 \n",
            " [============================>.] | Loss: 0.741 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.540 | Acc: 81.633% (720/882) 28/28 \n",
            " [============================>.] | Loss: 0.729 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.525 | Acc: 80.839% (713/882) 28/28 \n",
            " [============================>.] | Loss: 0.733 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.543 | Acc: 79.932% (705/882) 28/28 \n",
            " [============================>.] | Loss: 0.747 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.544 | Acc: 81.066% (715/882) 28/28 \n",
            " [============================>.] | Loss: 0.733 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.515 | Acc: 81.859% (722/882) 28/28 \n",
            " [============================>.] | Loss: 0.752 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.532 | Acc: 80.726% (712/882) 28/28 \n",
            " [============================>.] | Loss: 0.722 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.512 | Acc: 81.066% (715/882) 28/28 \n",
            " [============================>.] | Loss: 0.724 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.534 | Acc: 80.952% (714/882) 28/28 \n",
            " [============================>.] | Loss: 0.739 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.528 | Acc: 80.385% (709/882) 28/28 \n",
            " [============================>.] | Loss: 0.733 | Acc: 72.727% (72/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.530 | Acc: 80.159% (707/882) 28/28 \n",
            " [============================>.] | Loss: 0.725 | Acc: 72.727% (72/99) 20/20 \n",
            "best_Test_acc: 75.758\n",
            "best_Test_acc_epoch: 34\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.299 | Acc: 25.170% (222/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 4.999 | Acc: 21.212% (21/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 21.212\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.155 | Acc: 33.333% (294/882) 28/28 \n",
            " [============================>.] | Loss: 2.026 | Acc: 23.232% (23/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 23.232\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.018 | Acc: 34.807% (307/882) 28/28 \n",
            " [============================>.] | Loss: 4.332 | Acc: 24.242% (24/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 24.242\n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.900 | Acc: 36.848% (325/882) 28/28 \n",
            " [============================>.] | Loss: 2.258 | Acc: 36.364% (36/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 36.364\n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.747 | Acc: 39.683% (350/882) 28/28 \n",
            " [============================>.] | Loss: 1.590 | Acc: 41.414% (41/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 41.414\n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.541 | Acc: 45.011% (397/882) 28/28 \n",
            " [============================>.] | Loss: 3.752 | Acc: 40.404% (40/99) 20/20 \n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.656 | Acc: 43.651% (385/882) 28/28 \n",
            " [============================>.] | Loss: 1.822 | Acc: 40.404% (40/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.640 | Acc: 42.857% (378/882) 28/28 \n",
            " [============================>.] | Loss: 1.630 | Acc: 45.455% (45/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 45.455\n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.322 | Acc: 52.834% (466/882) 28/28 \n",
            " [============================>.] | Loss: 1.236 | Acc: 56.566% (56/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 56.566\n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.171 | Acc: 58.503% (516/882) 28/28 \n",
            " [============================>.] | Loss: 1.544 | Acc: 50.505% (50/99) 20/20 \n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.110 | Acc: 61.791% (545/882) 28/28 \n",
            " [============================>.] | Loss: 1.172 | Acc: 57.576% (57/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 57.576\n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.124 | Acc: 59.864% (528/882) 28/28 \n",
            " [============================>.] | Loss: 1.478 | Acc: 39.394% (39/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.000 | Acc: 63.265% (558/882) 28/28 \n",
            " [============================>.] | Loss: 1.174 | Acc: 48.485% (48/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.933 | Acc: 63.492% (560/882) 28/28 \n",
            " [============================>.] | Loss: 1.511 | Acc: 45.455% (45/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.876 | Acc: 68.934% (608/882) 28/28 \n",
            " [============================>.] | Loss: 1.124 | Acc: 63.636% (63/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 63.636\n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.915 | Acc: 65.646% (579/882) 28/28 \n",
            " [============================>.] | Loss: 1.019 | Acc: 59.596% (59/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.951 | Acc: 63.152% (557/882) 28/28 \n",
            " [============================>.] | Loss: 1.016 | Acc: 60.606% (60/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.810 | Acc: 68.027% (600/882) 28/28 \n",
            " [============================>.] | Loss: 0.989 | Acc: 66.667% (66/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 66.667\n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.778 | Acc: 72.902% (643/882) 28/28 \n",
            " [============================>.] | Loss: 1.217 | Acc: 52.525% (52/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.685 | Acc: 71.995% (635/882) 28/28 \n",
            " [============================>.] | Loss: 0.798 | Acc: 71.717% (71/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 71.717\n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.621 | Acc: 73.923% (652/882) 28/28 \n",
            " [============================>.] | Loss: 0.652 | Acc: 67.677% (67/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 0.632 | Acc: 75.057% (662/882) 28/28 \n",
            " [============================>.] | Loss: 0.888 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.544 | Acc: 77.664% (685/882) 28/28 \n",
            " [============================>.] | Loss: 0.621 | Acc: 73.737% (73/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 73.737\n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.485 | Acc: 78.458% (692/882) 28/28 \n",
            " [============================>.] | Loss: 0.536 | Acc: 76.768% (76/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 76.768\n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.444 | Acc: 82.086% (724/882) 28/28 \n",
            " [============================>.] | Loss: 0.659 | Acc: 68.687% (68/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.387 | Acc: 83.220% (734/882) 28/28 \n",
            " [============================>.] | Loss: 0.753 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.395 | Acc: 83.787% (739/882) 28/28 \n",
            " [============================>.] | Loss: 0.732 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.343 | Acc: 84.921% (749/882) 28/28 \n",
            " [============================>.] | Loss: 0.489 | Acc: 81.818% (81/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 81.818\n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.333 | Acc: 85.941% (758/882) 28/28 \n",
            " [============================>.] | Loss: 0.482 | Acc: 76.768% (76/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.321 | Acc: 86.508% (763/882) 28/28 \n",
            " [============================>.] | Loss: 0.650 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.279 | Acc: 88.095% (777/882) 28/28 \n",
            " [============================>.] | Loss: 0.518 | Acc: 75.758% (75/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.370 | Acc: 88.662% (782/882) 28/28 \n",
            " [============================>.] | Loss: 0.568 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.260 | Acc: 88.889% (784/882) 28/28 \n",
            " [============================>.] | Loss: 0.614 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.230 | Acc: 90.023% (794/882) 28/28 \n",
            " [============================>.] | Loss: 0.509 | Acc: 84.848% (84/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 84.848\n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.219 | Acc: 90.363% (797/882) 28/28 \n",
            " [============================>.] | Loss: 0.497 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.212 | Acc: 91.837% (810/882) 28/28 \n",
            " [============================>.] | Loss: 0.508 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.210 | Acc: 91.156% (804/882) 28/28 \n",
            " [============================>.] | Loss: 0.447 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.250 | Acc: 90.249% (796/882) 28/28 \n",
            " [============================>.] | Loss: 0.489 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.209 | Acc: 91.610% (808/882) 28/28 \n",
            " [============================>.] | Loss: 0.526 | Acc: 85.859% (85/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 85.859\n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.220 | Acc: 90.590% (799/882) 28/28 \n",
            " [============================>.] | Loss: 0.455 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.230 | Acc: 91.270% (805/882) 28/28 \n",
            " [============================>.] | Loss: 0.471 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.227 | Acc: 90.476% (798/882) 28/28 \n",
            " [============================>.] | Loss: 0.475 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.207 | Acc: 91.043% (803/882) 28/28 \n",
            " [============================>.] | Loss: 0.468 | Acc: 86.869% (86/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 86.869\n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.224 | Acc: 90.136% (795/882) 28/28 \n",
            " [============================>.] | Loss: 0.498 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.195 | Acc: 92.744% (818/882) 28/28 \n",
            " [============================>.] | Loss: 0.485 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.215 | Acc: 92.063% (812/882) 28/28 \n",
            " [============================>.] | Loss: 0.464 | Acc: 87.879% (87/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 87.879\n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.204 | Acc: 91.156% (804/882) 28/28 \n",
            " [============================>.] | Loss: 0.489 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.200 | Acc: 92.063% (812/882) 28/28 \n",
            " [============================>.] | Loss: 0.491 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.197 | Acc: 91.156% (804/882) 28/28 \n",
            " [============================>.] | Loss: 0.520 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.211 | Acc: 90.590% (799/882) 28/28 \n",
            " [============================>.] | Loss: 0.492 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.208 | Acc: 90.476% (798/882) 28/28 \n",
            " [============================>.] | Loss: 0.469 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.205 | Acc: 92.063% (812/882) 28/28 \n",
            " [============================>.] | Loss: 0.529 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.235 | Acc: 90.363% (797/882) 28/28 \n",
            " [============================>.] | Loss: 0.498 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.211 | Acc: 91.156% (804/882) 28/28 \n",
            " [============================>.] | Loss: 0.480 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.263 | Acc: 89.342% (788/882) 28/28 \n",
            " [============================>.] | Loss: 0.471 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.232 | Acc: 90.930% (802/882) 28/28 \n",
            " [============================>.] | Loss: 0.534 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.224 | Acc: 90.023% (794/882) 28/28 \n",
            " [============================>.] | Loss: 0.462 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.210 | Acc: 91.497% (807/882) 28/28 \n",
            " [============================>.] | Loss: 0.487 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.198 | Acc: 92.857% (819/882) 28/28 \n",
            " [============================>.] | Loss: 0.496 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.190 | Acc: 92.857% (819/882) 28/28 \n",
            " [============================>.] | Loss: 0.482 | Acc: 86.869% (86/99) 20/20 \n",
            "best_Test_acc: 87.879\n",
            "best_Test_acc_epoch: 45\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.982 | Acc: 33.560% (296/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 2.855 | Acc: 15.152% (15/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 15.152\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.756 | Acc: 39.342% (347/882) 28/28 \n",
            " [============================>.] | Loss: 1.404 | Acc: 49.495% (49/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 49.495\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.256 | Acc: 56.803% (501/882) 28/28 \n",
            " [============================>.] | Loss: 1.262 | Acc: 58.586% (58/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 58.586\n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.966 | Acc: 64.512% (569/882) 28/28 \n",
            " [============================>.] | Loss: 1.789 | Acc: 59.596% (59/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 59.596\n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.115 | Acc: 63.379% (559/882) 28/28 \n",
            " [============================>.] | Loss: 5.136 | Acc: 46.465% (46/99) 20/20 \n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.154 | Acc: 58.617% (517/882) 28/28 \n",
            " [============================>.] | Loss: 1.831 | Acc: 43.434% (43/99) 20/20 \n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.988 | Acc: 62.018% (547/882) 28/28 \n",
            " [============================>.] | Loss: 1.136 | Acc: 56.566% (56/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.882 | Acc: 64.399% (568/882) 28/28 \n",
            " [============================>.] | Loss: 1.250 | Acc: 60.606% (60/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 60.606\n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.815 | Acc: 68.367% (603/882) 28/28 \n",
            " [============================>.] | Loss: 1.738 | Acc: 63.636% (63/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 63.636\n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.761 | Acc: 68.481% (604/882) 28/28 \n",
            " [============================>.] | Loss: 1.545 | Acc: 55.556% (55/99) 20/20 \n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.684 | Acc: 72.449% (639/882) 28/28 \n",
            " [============================>.] | Loss: 1.062 | Acc: 61.616% (61/99) 20/20 \n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.792 | Acc: 70.181% (619/882) 28/28 \n",
            " [============================>.] | Loss: 1.415 | Acc: 69.697% (69/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 69.697\n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.690 | Acc: 71.655% (632/882) 28/28 \n",
            " [============================>.] | Loss: 1.168 | Acc: 57.576% (57/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.558 | Acc: 76.077% (671/882) 28/28 \n",
            " [============================>.] | Loss: 0.864 | Acc: 58.586% (58/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.515 | Acc: 79.365% (700/882) 28/28 \n",
            " [============================>.] | Loss: 0.933 | Acc: 69.697% (69/99) 20/20 \n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.505 | Acc: 80.612% (711/882) 28/28 \n",
            " [============================>.] | Loss: 1.030 | Acc: 77.778% (77/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 77.778\n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.566 | Acc: 76.644% (676/882) 28/28 \n",
            " [============================>.] | Loss: 0.748 | Acc: 69.697% (69/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.533 | Acc: 78.571% (693/882) 28/28 \n",
            " [============================>.] | Loss: 2.093 | Acc: 69.697% (69/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.499 | Acc: 80.045% (706/882) 28/28 \n",
            " [============================>.] | Loss: 0.544 | Acc: 77.778% (77/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.364 | Acc: 84.580% (746/882) 28/28 \n",
            " [============================>.] | Loss: 0.765 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.418 | Acc: 83.673% (738/882) 28/28 \n",
            " [============================>.] | Loss: 0.842 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 0.410 | Acc: 84.354% (744/882) 28/28 \n",
            " [============================>.] | Loss: 0.546 | Acc: 75.758% (75/99) 20/20 \n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.340 | Acc: 86.621% (764/882) 28/28 \n",
            " [============================>.] | Loss: 0.565 | Acc: 76.768% (76/99) 20/20 \n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.231 | Acc: 89.909% (793/882) 28/28 \n",
            " [============================>.] | Loss: 0.483 | Acc: 80.808% (80/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 80.808\n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.244 | Acc: 90.136% (795/882) 28/28 \n",
            " [============================>.] | Loss: 0.575 | Acc: 76.768% (76/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.189 | Acc: 92.857% (819/882) 28/28 \n",
            " [============================>.] | Loss: 0.384 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.149 | Acc: 94.898% (837/882) 28/28 \n",
            " [============================>.] | Loss: 0.417 | Acc: 76.768% (76/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.140 | Acc: 95.465% (842/882) 28/28 \n",
            " [============================>.] | Loss: 0.507 | Acc: 77.778% (77/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.127 | Acc: 94.558% (834/882) 28/28 \n",
            " [============================>.] | Loss: 0.523 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.153 | Acc: 95.351% (841/882) 28/28 \n",
            " [============================>.] | Loss: 0.546 | Acc: 75.758% (75/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.102 | Acc: 96.939% (855/882) 28/28 \n",
            " [============================>.] | Loss: 0.517 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.090 | Acc: 97.392% (859/882) 28/28 \n",
            " [============================>.] | Loss: 0.483 | Acc: 81.818% (81/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 81.818\n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.074 | Acc: 98.186% (866/882) 28/28 \n",
            " [============================>.] | Loss: 0.524 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.083 | Acc: 97.732% (862/882) 28/28 \n",
            " [============================>.] | Loss: 0.519 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.073 | Acc: 97.959% (864/882) 28/28 \n",
            " [============================>.] | Loss: 0.496 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.071 | Acc: 97.959% (864/882) 28/28 \n",
            " [============================>.] | Loss: 0.516 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.078 | Acc: 97.846% (863/882) 28/28 \n",
            " [============================>.] | Loss: 0.502 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.066 | Acc: 98.413% (868/882) 28/28 \n",
            " [============================>.] | Loss: 0.504 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.077 | Acc: 97.846% (863/882) 28/28 \n",
            " [============================>.] | Loss: 0.500 | Acc: 82.828% (82/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 82.828\n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.071 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.495 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.068 | Acc: 98.073% (865/882) 28/28 \n",
            " [============================>.] | Loss: 0.499 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.073 | Acc: 98.413% (868/882) 28/28 \n",
            " [============================>.] | Loss: 0.524 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.068 | Acc: 98.073% (865/882) 28/28 \n",
            " [============================>.] | Loss: 0.526 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.079 | Acc: 98.073% (865/882) 28/28 \n",
            " [============================>.] | Loss: 0.507 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.068 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.510 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.069 | Acc: 98.073% (865/882) 28/28 \n",
            " [============================>.] | Loss: 0.509 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.055 | Acc: 98.753% (871/882) 28/28 \n",
            " [============================>.] | Loss: 0.513 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.076 | Acc: 97.506% (860/882) 28/28 \n",
            " [============================>.] | Loss: 0.527 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.061 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.506 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.070 | Acc: 97.959% (864/882) 28/28 \n",
            " [============================>.] | Loss: 0.521 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.073 | Acc: 98.073% (865/882) 28/28 \n",
            " [============================>.] | Loss: 0.523 | Acc: 83.838% (83/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 83.838\n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.062 | Acc: 98.413% (868/882) 28/28 \n",
            " [============================>.] | Loss: 0.517 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.067 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.514 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.058 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.510 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.076 | Acc: 97.732% (862/882) 28/28 \n",
            " [============================>.] | Loss: 0.504 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.070 | Acc: 97.506% (860/882) 28/28 \n",
            " [============================>.] | Loss: 0.529 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.076 | Acc: 98.299% (867/882) 28/28 \n",
            " [============================>.] | Loss: 0.525 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.066 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.516 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.077 | Acc: 97.846% (863/882) 28/28 \n",
            " [============================>.] | Loss: 0.518 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.053 | Acc: 99.093% (874/882) 28/28 \n",
            " [============================>.] | Loss: 0.512 | Acc: 82.828% (82/99) 20/20 \n",
            "best_Test_acc: 83.838\n",
            "best_Test_acc_epoch: 50\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.230 | Acc: 21.542% (190/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 1.892 | Acc: 18.182% (18/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 18.182\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.070 | Acc: 34.354% (303/882) 28/28 \n",
            " [============================>.] | Loss: 1.824 | Acc: 30.303% (30/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 30.303\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.853 | Acc: 34.127% (301/882) 28/28 \n",
            " [============================>.] | Loss: 1.803 | Acc: 30.303% (30/99) 20/20 \n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.684 | Acc: 38.095% (336/882) 28/28 \n",
            " [============================>.] | Loss: 1.865 | Acc: 27.273% (27/99) 20/20 \n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.678 | Acc: 40.930% (361/882) 28/28 \n",
            " [============================>.] | Loss: 1.747 | Acc: 33.333% (33/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 33.333\n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.555 | Acc: 41.497% (366/882) 28/28 \n",
            " [============================>.] | Loss: 1.506 | Acc: 40.404% (40/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 40.404\n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.464 | Acc: 46.259% (408/882) 28/28 \n",
            " [============================>.] | Loss: 1.367 | Acc: 47.475% (47/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 47.475\n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.296 | Acc: 56.463% (498/882) 28/28 \n",
            " [============================>.] | Loss: 2.347 | Acc: 26.263% (26/99) 20/20 \n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.314 | Acc: 55.782% (492/882) 28/28 \n",
            " [============================>.] | Loss: 1.330 | Acc: 51.515% (51/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 51.515\n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.160 | Acc: 60.658% (535/882) 28/28 \n",
            " [============================>.] | Loss: 1.268 | Acc: 53.535% (53/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 53.535\n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.103 | Acc: 61.224% (540/882) 28/28 \n",
            " [============================>.] | Loss: 0.958 | Acc: 71.717% (71/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 71.717\n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.922 | Acc: 64.966% (573/882) 28/28 \n",
            " [============================>.] | Loss: 0.930 | Acc: 59.596% (59/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.952 | Acc: 64.853% (572/882) 28/28 \n",
            " [============================>.] | Loss: 1.204 | Acc: 52.525% (52/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.956 | Acc: 65.760% (580/882) 28/28 \n",
            " [============================>.] | Loss: 1.090 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.873 | Acc: 69.501% (613/882) 28/28 \n",
            " [============================>.] | Loss: 1.001 | Acc: 61.616% (61/99) 20/20 \n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.872 | Acc: 66.327% (585/882) 28/28 \n",
            " [============================>.] | Loss: 0.790 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.851 | Acc: 66.327% (585/882) 28/28 \n",
            " [============================>.] | Loss: 1.008 | Acc: 69.697% (69/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.887 | Acc: 67.800% (598/882) 28/28 \n",
            " [============================>.] | Loss: 0.923 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.771 | Acc: 70.181% (619/882) 28/28 \n",
            " [============================>.] | Loss: 1.053 | Acc: 55.556% (55/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.688 | Acc: 73.696% (650/882) 28/28 \n",
            " [============================>.] | Loss: 0.807 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.693 | Acc: 73.356% (647/882) 28/28 \n",
            " [============================>.] | Loss: 1.322 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 0.648 | Acc: 75.964% (670/882) 28/28 \n",
            " [============================>.] | Loss: 0.688 | Acc: 71.717% (71/99) 20/20 \n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.599 | Acc: 76.531% (675/882) 28/28 \n",
            " [============================>.] | Loss: 0.508 | Acc: 72.727% (72/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 72.727\n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.513 | Acc: 79.025% (697/882) 28/28 \n",
            " [============================>.] | Loss: 0.521 | Acc: 83.838% (83/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 83.838\n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.447 | Acc: 83.220% (734/882) 28/28 \n",
            " [============================>.] | Loss: 0.585 | Acc: 74.747% (74/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.429 | Acc: 82.653% (729/882) 28/28 \n",
            " [============================>.] | Loss: 0.595 | Acc: 75.758% (75/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.363 | Acc: 84.694% (747/882) 28/28 \n",
            " [============================>.] | Loss: 0.529 | Acc: 77.778% (77/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.365 | Acc: 85.261% (752/882) 28/28 \n",
            " [============================>.] | Loss: 0.569 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.345 | Acc: 86.168% (760/882) 28/28 \n",
            " [============================>.] | Loss: 0.423 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.291 | Acc: 87.982% (776/882) 28/28 \n",
            " [============================>.] | Loss: 0.432 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.277 | Acc: 88.435% (780/882) 28/28 \n",
            " [============================>.] | Loss: 0.424 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.277 | Acc: 87.528% (772/882) 28/28 \n",
            " [============================>.] | Loss: 0.465 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.256 | Acc: 90.136% (795/882) 28/28 \n",
            " [============================>.] | Loss: 0.423 | Acc: 84.848% (84/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 84.848\n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.256 | Acc: 90.703% (800/882) 28/28 \n",
            " [============================>.] | Loss: 0.447 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.255 | Acc: 88.776% (783/882) 28/28 \n",
            " [============================>.] | Loss: 0.461 | Acc: 85.859% (85/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 85.859\n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.263 | Acc: 88.549% (781/882) 28/28 \n",
            " [============================>.] | Loss: 0.446 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.237 | Acc: 91.156% (804/882) 28/28 \n",
            " [============================>.] | Loss: 0.466 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.236 | Acc: 89.909% (793/882) 28/28 \n",
            " [============================>.] | Loss: 0.489 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.237 | Acc: 90.476% (798/882) 28/28 \n",
            " [============================>.] | Loss: 0.545 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.208 | Acc: 91.497% (807/882) 28/28 \n",
            " [============================>.] | Loss: 0.482 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.242 | Acc: 90.816% (801/882) 28/28 \n",
            " [============================>.] | Loss: 0.528 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.233 | Acc: 91.043% (803/882) 28/28 \n",
            " [============================>.] | Loss: 0.522 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.222 | Acc: 90.023% (794/882) 28/28 \n",
            " [============================>.] | Loss: 0.519 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.227 | Acc: 89.342% (788/882) 28/28 \n",
            " [============================>.] | Loss: 0.519 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.224 | Acc: 90.476% (798/882) 28/28 \n",
            " [============================>.] | Loss: 0.487 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.240 | Acc: 89.909% (793/882) 28/28 \n",
            " [============================>.] | Loss: 0.495 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.207 | Acc: 91.383% (806/882) 28/28 \n",
            " [============================>.] | Loss: 0.471 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.234 | Acc: 90.816% (801/882) 28/28 \n",
            " [============================>.] | Loss: 0.509 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.247 | Acc: 89.796% (792/882) 28/28 \n",
            " [============================>.] | Loss: 0.501 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.226 | Acc: 89.909% (793/882) 28/28 \n",
            " [============================>.] | Loss: 0.516 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.255 | Acc: 89.683% (791/882) 28/28 \n",
            " [============================>.] | Loss: 0.526 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.224 | Acc: 91.383% (806/882) 28/28 \n",
            " [============================>.] | Loss: 0.485 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.211 | Acc: 91.723% (809/882) 28/28 \n",
            " [============================>.] | Loss: 0.493 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.239 | Acc: 89.796% (792/882) 28/28 \n",
            " [============================>.] | Loss: 0.499 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.214 | Acc: 92.290% (814/882) 28/28 \n",
            " [============================>.] | Loss: 0.501 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.210 | Acc: 91.383% (806/882) 28/28 \n",
            " [============================>.] | Loss: 0.483 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.233 | Acc: 90.023% (794/882) 28/28 \n",
            " [============================>.] | Loss: 0.479 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.222 | Acc: 91.270% (805/882) 28/28 \n",
            " [============================>.] | Loss: 0.487 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.225 | Acc: 90.476% (798/882) 28/28 \n",
            " [============================>.] | Loss: 0.489 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.309 | Acc: 89.342% (788/882) 28/28 \n",
            " [============================>.] | Loss: 0.476 | Acc: 80.808% (80/99) 20/20 \n",
            "best_Test_acc: 85.859\n",
            "best_Test_acc_epoch: 34\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.324 | Acc: 20.862% (184/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 1.866 | Acc: 24.242% (24/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 24.242\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.354 | Acc: 20.295% (179/882) 28/28 \n",
            " [============================>.] | Loss: 4.409 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.345 | Acc: 21.542% (190/882) 28/28 \n",
            " [============================>.] | Loss: 2.154 | Acc: 21.212% (21/99) 20/20 \n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.331 | Acc: 21.882% (193/882) 28/28 \n",
            " [============================>.] | Loss: 1.871 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.328 | Acc: 23.129% (204/882) 28/28 \n",
            " [============================>.] | Loss: 1.879 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.200 | Acc: 22.676% (200/882) 28/28 \n",
            " [============================>.] | Loss: 33.968 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.970 | Acc: 29.025% (256/882) 28/28 \n",
            " [============================>.] | Loss: 1.475 | Acc: 42.424% (42/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 42.424\n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.571 | Acc: 37.982% (335/882) 28/28 \n",
            " [============================>.] | Loss: 1.635 | Acc: 36.364% (36/99) 20/20 \n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.458 | Acc: 40.363% (356/882) 28/28 \n",
            " [============================>.] | Loss: 1.355 | Acc: 42.424% (42/99) 20/20 \n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.406 | Acc: 40.703% (359/882) 28/28 \n",
            " [============================>.] | Loss: 1.353 | Acc: 45.455% (45/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 45.455\n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.417 | Acc: 41.383% (365/882) 28/28 \n",
            " [============================>.] | Loss: 1.383 | Acc: 41.414% (41/99) 20/20 \n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.371 | Acc: 42.744% (377/882) 28/28 \n",
            " [============================>.] | Loss: 1.333 | Acc: 45.455% (45/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.355 | Acc: 42.290% (373/882) 28/28 \n",
            " [============================>.] | Loss: 1.337 | Acc: 42.424% (42/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.341 | Acc: 42.517% (375/882) 28/28 \n",
            " [============================>.] | Loss: 1.343 | Acc: 42.424% (42/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.326 | Acc: 44.785% (395/882) 28/28 \n",
            " [============================>.] | Loss: 1.301 | Acc: 43.434% (43/99) 20/20 \n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.335 | Acc: 42.744% (377/882) 28/28 \n",
            " [============================>.] | Loss: 1.317 | Acc: 45.455% (45/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.326 | Acc: 42.517% (375/882) 28/28 \n",
            " [============================>.] | Loss: 1.328 | Acc: 45.455% (45/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.251 | Acc: 46.825% (413/882) 28/28 \n",
            " [============================>.] | Loss: 1.325 | Acc: 41.414% (41/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.246 | Acc: 49.773% (439/882) 28/28 \n",
            " [============================>.] | Loss: 1.282 | Acc: 41.414% (41/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.204 | Acc: 51.814% (457/882) 28/28 \n",
            " [============================>.] | Loss: 1.276 | Acc: 49.495% (49/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 49.495\n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.228 | Acc: 50.567% (446/882) 28/28 \n",
            " [============================>.] | Loss: 1.289 | Acc: 45.455% (45/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 1.145 | Acc: 53.401% (471/882) 28/28 \n",
            " [============================>.] | Loss: 1.131 | Acc: 55.556% (55/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 55.556\n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 1.017 | Acc: 58.730% (518/882) 28/28 \n",
            " [============================>.] | Loss: 1.119 | Acc: 56.566% (56/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 56.566\n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.941 | Acc: 62.358% (550/882) 28/28 \n",
            " [============================>.] | Loss: 1.220 | Acc: 52.525% (52/99) 20/20 \n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.926 | Acc: 63.039% (556/882) 28/28 \n",
            " [============================>.] | Loss: 0.986 | Acc: 65.657% (65/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 65.657\n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.853 | Acc: 66.440% (586/882) 28/28 \n",
            " [============================>.] | Loss: 1.000 | Acc: 68.687% (68/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 68.687\n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.823 | Acc: 66.100% (583/882) 28/28 \n",
            " [============================>.] | Loss: 1.232 | Acc: 60.606% (60/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.763 | Acc: 69.161% (610/882) 28/28 \n",
            " [============================>.] | Loss: 1.071 | Acc: 58.586% (58/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.717 | Acc: 71.769% (633/882) 28/28 \n",
            " [============================>.] | Loss: 1.033 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.708 | Acc: 71.995% (635/882) 28/28 \n",
            " [============================>.] | Loss: 0.986 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.718 | Acc: 70.295% (620/882) 28/28 \n",
            " [============================>.] | Loss: 1.026 | Acc: 60.606% (60/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.650 | Acc: 73.696% (650/882) 28/28 \n",
            " [============================>.] | Loss: 1.007 | Acc: 58.586% (58/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.654 | Acc: 72.562% (640/882) 28/28 \n",
            " [============================>.] | Loss: 0.940 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.625 | Acc: 75.057% (662/882) 28/28 \n",
            " [============================>.] | Loss: 1.030 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.635 | Acc: 75.510% (666/882) 28/28 \n",
            " [============================>.] | Loss: 0.960 | Acc: 60.606% (60/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.605 | Acc: 75.057% (662/882) 28/28 \n",
            " [============================>.] | Loss: 0.954 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.601 | Acc: 76.417% (674/882) 28/28 \n",
            " [============================>.] | Loss: 0.926 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.602 | Acc: 76.757% (677/882) 28/28 \n",
            " [============================>.] | Loss: 0.921 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.594 | Acc: 75.283% (664/882) 28/28 \n",
            " [============================>.] | Loss: 0.996 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.584 | Acc: 76.984% (679/882) 28/28 \n",
            " [============================>.] | Loss: 0.961 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.593 | Acc: 77.098% (680/882) 28/28 \n",
            " [============================>.] | Loss: 0.949 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.604 | Acc: 77.098% (680/882) 28/28 \n",
            " [============================>.] | Loss: 1.047 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.591 | Acc: 75.737% (668/882) 28/28 \n",
            " [============================>.] | Loss: 0.974 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.552 | Acc: 77.438% (683/882) 28/28 \n",
            " [============================>.] | Loss: 0.945 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.583 | Acc: 78.118% (689/882) 28/28 \n",
            " [============================>.] | Loss: 0.979 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.596 | Acc: 75.283% (664/882) 28/28 \n",
            " [============================>.] | Loss: 0.978 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.577 | Acc: 78.458% (692/882) 28/28 \n",
            " [============================>.] | Loss: 1.009 | Acc: 61.616% (61/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.591 | Acc: 76.417% (674/882) 28/28 \n",
            " [============================>.] | Loss: 0.986 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.597 | Acc: 76.190% (672/882) 28/28 \n",
            " [============================>.] | Loss: 0.996 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.567 | Acc: 77.324% (682/882) 28/28 \n",
            " [============================>.] | Loss: 0.955 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.604 | Acc: 76.531% (675/882) 28/28 \n",
            " [============================>.] | Loss: 1.005 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.567 | Acc: 78.231% (690/882) 28/28 \n",
            " [============================>.] | Loss: 0.974 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.573 | Acc: 78.118% (689/882) 28/28 \n",
            " [============================>.] | Loss: 1.009 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.618 | Acc: 74.830% (660/882) 28/28 \n",
            " [============================>.] | Loss: 0.948 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.582 | Acc: 76.417% (674/882) 28/28 \n",
            " [============================>.] | Loss: 0.991 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.612 | Acc: 75.057% (662/882) 28/28 \n",
            " [============================>.] | Loss: 0.971 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.567 | Acc: 78.005% (688/882) 28/28 \n",
            " [============================>.] | Loss: 0.976 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.579 | Acc: 78.458% (692/882) 28/28 \n",
            " [============================>.] | Loss: 0.984 | Acc: 62.626% (62/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.566 | Acc: 78.345% (691/882) 28/28 \n",
            " [============================>.] | Loss: 0.991 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.558 | Acc: 79.138% (698/882) 28/28 \n",
            " [============================>.] | Loss: 0.962 | Acc: 63.636% (63/99) 20/20 \n",
            "best_Test_acc: 68.687\n",
            "best_Test_acc_epoch: 25\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.266 | Acc: 21.429% (189/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 1.866 | Acc: 18.182% (18/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 18.182\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.250 | Acc: 22.222% (196/882) 28/28 \n",
            " [============================>.] | Loss: 1.861 | Acc: 24.242% (24/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 24.242\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.450 | Acc: 23.243% (205/882) 28/28 \n",
            " [============================>.] | Loss: 2.307 | Acc: 33.333% (33/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 33.333\n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.423 | Acc: 20.068% (177/882) 28/28 \n",
            " [============================>.] | Loss: 1.939 | Acc: 25.253% (25/99) 20/20 \n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.383 | Acc: 22.902% (202/882) 28/28 \n",
            " [============================>.] | Loss: 1.893 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.198 | Acc: 22.336% (197/882) 28/28 \n",
            " [============================>.] | Loss: 1.915 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.278 | Acc: 21.429% (189/882) 28/28 \n",
            " [============================>.] | Loss: 1.859 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.295 | Acc: 23.356% (206/882) 28/28 \n",
            " [============================>.] | Loss: 2.131 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.099 | Acc: 22.789% (201/882) 28/28 \n",
            " [============================>.] | Loss: 1.887 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.479 | Acc: 25.624% (226/882) 28/28 \n",
            " [============================>.] | Loss: 2.946 | Acc: 21.212% (21/99) 20/20 \n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.320 | Acc: 21.542% (190/882) 28/28 \n",
            " [============================>.] | Loss: 1.900 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.250 | Acc: 22.222% (196/882) 28/28 \n",
            " [============================>.] | Loss: 1.865 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.200 | Acc: 23.129% (204/882) 28/28 \n",
            " [============================>.] | Loss: 1.857 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.243 | Acc: 23.129% (204/882) 28/28 \n",
            " [============================>.] | Loss: 1.894 | Acc: 22.222% (22/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.137 | Acc: 21.995% (194/882) 28/28 \n",
            " [============================>.] | Loss: 1.865 | Acc: 23.232% (23/99) 20/20 \n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.216 | Acc: 22.336% (197/882) 28/28 \n",
            " [============================>.] | Loss: 1.847 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.117 | Acc: 24.717% (218/882) 28/28 \n",
            " [============================>.] | Loss: 1.867 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.135 | Acc: 23.469% (207/882) 28/28 \n",
            " [============================>.] | Loss: 1.888 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.168 | Acc: 25.510% (225/882) 28/28 \n",
            " [============================>.] | Loss: 1.862 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.066 | Acc: 23.129% (204/882) 28/28 \n",
            " [============================>.] | Loss: 1.859 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.019 | Acc: 23.583% (208/882) 28/28 \n",
            " [============================>.] | Loss: 1.860 | Acc: 22.222% (22/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 2.097 | Acc: 23.129% (204/882) 28/28 \n",
            " [============================>.] | Loss: 1.860 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 1.998 | Acc: 23.016% (203/882) 28/28 \n",
            " [============================>.] | Loss: 1.861 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 2.061 | Acc: 24.490% (216/882) 28/28 \n",
            " [============================>.] | Loss: 1.858 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 2.042 | Acc: 24.376% (215/882) 28/28 \n",
            " [============================>.] | Loss: 1.856 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 2.046 | Acc: 22.676% (200/882) 28/28 \n",
            " [============================>.] | Loss: 1.851 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 2.005 | Acc: 24.603% (217/882) 28/28 \n",
            " [============================>.] | Loss: 1.854 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 1.998 | Acc: 24.717% (218/882) 28/28 \n",
            " [============================>.] | Loss: 1.850 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 1.961 | Acc: 25.624% (226/882) 28/28 \n",
            " [============================>.] | Loss: 1.856 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 1.952 | Acc: 24.830% (219/882) 28/28 \n",
            " [============================>.] | Loss: 1.832 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 1.973 | Acc: 23.469% (207/882) 28/28 \n",
            " [============================>.] | Loss: 1.825 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 1.943 | Acc: 24.263% (214/882) 28/28 \n",
            " [============================>.] | Loss: 1.835 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 1.977 | Acc: 23.243% (205/882) 28/28 \n",
            " [============================>.] | Loss: 1.845 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 1.914 | Acc: 26.757% (236/882) 28/28 \n",
            " [============================>.] | Loss: 1.830 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 1.928 | Acc: 24.603% (217/882) 28/28 \n",
            " [============================>.] | Loss: 1.850 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 1.968 | Acc: 23.016% (203/882) 28/28 \n",
            " [============================>.] | Loss: 1.847 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 1.933 | Acc: 23.129% (204/882) 28/28 \n",
            " [============================>.] | Loss: 1.844 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 1.905 | Acc: 25.170% (222/882) 28/28 \n",
            " [============================>.] | Loss: 1.852 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 1.900 | Acc: 25.510% (225/882) 28/28 \n",
            " [============================>.] | Loss: 1.848 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 1.925 | Acc: 24.717% (218/882) 28/28 \n",
            " [============================>.] | Loss: 1.831 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 1.955 | Acc: 25.964% (229/882) 28/28 \n",
            " [============================>.] | Loss: 1.844 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 1.971 | Acc: 24.263% (214/882) 28/28 \n",
            " [============================>.] | Loss: 1.830 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 1.916 | Acc: 24.943% (220/882) 28/28 \n",
            " [============================>.] | Loss: 1.849 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 1.911 | Acc: 26.077% (230/882) 28/28 \n",
            " [============================>.] | Loss: 1.849 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 1.937 | Acc: 24.376% (215/882) 28/28 \n",
            " [============================>.] | Loss: 1.843 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 1.966 | Acc: 24.603% (217/882) 28/28 \n",
            " [============================>.] | Loss: 1.854 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 1.906 | Acc: 25.283% (223/882) 28/28 \n",
            " [============================>.] | Loss: 1.850 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 1.929 | Acc: 26.417% (233/882) 28/28 \n",
            " [============================>.] | Loss: 1.847 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 1.943 | Acc: 24.717% (218/882) 28/28 \n",
            " [============================>.] | Loss: 1.840 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 1.945 | Acc: 24.263% (214/882) 28/28 \n",
            " [============================>.] | Loss: 1.841 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 1.949 | Acc: 24.603% (217/882) 28/28 \n",
            " [============================>.] | Loss: 1.846 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 1.949 | Acc: 24.490% (216/882) 28/28 \n",
            " [============================>.] | Loss: 1.821 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 1.927 | Acc: 25.510% (225/882) 28/28 \n",
            " [============================>.] | Loss: 1.834 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 1.972 | Acc: 24.376% (215/882) 28/28 \n",
            " [============================>.] | Loss: 1.847 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 1.943 | Acc: 22.789% (201/882) 28/28 \n",
            " [============================>.] | Loss: 1.833 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 1.949 | Acc: 24.490% (216/882) 28/28 \n",
            " [============================>.] | Loss: 1.836 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 1.907 | Acc: 25.057% (221/882) 28/28 \n",
            " [============================>.] | Loss: 1.851 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 1.939 | Acc: 24.490% (216/882) 28/28 \n",
            " [============================>.] | Loss: 1.850 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 1.911 | Acc: 26.077% (230/882) 28/28 \n",
            " [============================>.] | Loss: 1.851 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 1.926 | Acc: 23.923% (211/882) 28/28 \n",
            " [============================>.] | Loss: 1.846 | Acc: 24.242% (24/99) 20/20 \n",
            "best_Test_acc: 33.333\n",
            "best_Test_acc_epoch: 2\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.738 | Acc: 34.694% (306/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 11.409 | Acc: 30.303% (30/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 30.303\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.476 | Acc: 41.837% (369/882) 28/28 \n",
            " [============================>.] | Loss: 2.015 | Acc: 44.444% (44/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 44.444\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.373 | Acc: 46.145% (407/882) 28/28 \n",
            " [============================>.] | Loss: 1.322 | Acc: 50.505% (50/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 50.505\n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.285 | Acc: 47.959% (423/882) 28/28 \n",
            " [============================>.] | Loss: 1.351 | Acc: 42.424% (42/99) 20/20 \n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.280 | Acc: 47.506% (419/882) 28/28 \n",
            " [============================>.] | Loss: 1.244 | Acc: 50.505% (50/99) 20/20 \n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.229 | Acc: 51.247% (452/882) 28/28 \n",
            " [============================>.] | Loss: 1.202 | Acc: 55.556% (55/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 55.556\n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.101 | Acc: 54.422% (480/882) 28/28 \n",
            " [============================>.] | Loss: 1.100 | Acc: 51.515% (51/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.051 | Acc: 58.844% (519/882) 28/28 \n",
            " [============================>.] | Loss: 1.393 | Acc: 48.485% (48/99) 20/20 \n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.887 | Acc: 64.399% (568/882) 28/28 \n",
            " [============================>.] | Loss: 0.818 | Acc: 67.677% (67/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 67.677\n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.741 | Acc: 71.882% (634/882) 28/28 \n",
            " [============================>.] | Loss: 0.725 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.655 | Acc: 73.016% (644/882) 28/28 \n",
            " [============================>.] | Loss: 0.589 | Acc: 76.768% (76/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 76.768\n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.641 | Acc: 77.664% (685/882) 28/28 \n",
            " [============================>.] | Loss: 0.542 | Acc: 76.768% (76/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.665 | Acc: 73.810% (651/882) 28/28 \n",
            " [============================>.] | Loss: 1.663 | Acc: 64.646% (64/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.767 | Acc: 70.408% (621/882) 28/28 \n",
            " [============================>.] | Loss: 0.563 | Acc: 77.778% (77/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 77.778\n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.503 | Acc: 80.726% (712/882) 28/28 \n",
            " [============================>.] | Loss: 0.705 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.504 | Acc: 81.406% (718/882) 28/28 \n",
            " [============================>.] | Loss: 1.014 | Acc: 70.707% (70/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.380 | Acc: 84.921% (749/882) 28/28 \n",
            " [============================>.] | Loss: 0.429 | Acc: 86.869% (86/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 86.869\n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.315 | Acc: 88.549% (781/882) 28/28 \n",
            " [============================>.] | Loss: 0.553 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.334 | Acc: 87.302% (770/882) 28/28 \n",
            " [============================>.] | Loss: 0.569 | Acc: 78.788% (78/99) 20/20 \n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.318 | Acc: 89.002% (785/882) 28/28 \n",
            " [============================>.] | Loss: 0.601 | Acc: 81.818% (81/99) 20/20 \n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.308 | Acc: 89.569% (790/882) 28/28 \n",
            " [============================>.] | Loss: 0.360 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 0.148 | Acc: 95.692% (844/882) 28/28 \n",
            " [============================>.] | Loss: 0.360 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.108 | Acc: 96.599% (852/882) 28/28 \n",
            " [============================>.] | Loss: 0.411 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.082 | Acc: 97.392% (859/882) 28/28 \n",
            " [============================>.] | Loss: 0.273 | Acc: 91.919% (91/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 91.919\n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.070 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.259 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.056 | Acc: 98.639% (870/882) 28/28 \n",
            " [============================>.] | Loss: 0.286 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.072 | Acc: 98.186% (866/882) 28/28 \n",
            " [============================>.] | Loss: 0.233 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.051 | Acc: 98.526% (869/882) 28/28 \n",
            " [============================>.] | Loss: 0.257 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.035 | Acc: 99.320% (876/882) 28/28 \n",
            " [============================>.] | Loss: 0.314 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.044 | Acc: 98.980% (873/882) 28/28 \n",
            " [============================>.] | Loss: 0.312 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.028 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.289 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.028 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.243 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.019 | Acc: 99.660% (879/882) 28/28 \n",
            " [============================>.] | Loss: 0.236 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.029 | Acc: 99.320% (876/882) 28/28 \n",
            " [============================>.] | Loss: 0.289 | Acc: 88.889% (88/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.032 | Acc: 99.206% (875/882) 28/28 \n",
            " [============================>.] | Loss: 0.305 | Acc: 86.869% (86/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.022 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.221 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.023 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.198 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.021 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.267 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.018 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.245 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.021 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.230 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.022 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.230 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.023 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.228 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.023 | Acc: 99.320% (876/882) 28/28 \n",
            " [============================>.] | Loss: 0.291 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.019 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.254 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.025 | Acc: 99.206% (875/882) 28/28 \n",
            " [============================>.] | Loss: 0.240 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.018 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.229 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.020 | Acc: 99.660% (879/882) 28/28 \n",
            " [============================>.] | Loss: 0.259 | Acc: 87.879% (87/99) 20/20 \n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.018 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.249 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.018 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.238 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.020 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.242 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.026 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.218 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.023 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.235 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.023 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.245 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.023 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.246 | Acc: 90.909% (90/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.017 | Acc: 99.660% (879/882) 28/28 \n",
            " [============================>.] | Loss: 0.248 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.021 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.233 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.025 | Acc: 99.546% (878/882) 28/28 \n",
            " [============================>.] | Loss: 0.230 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.016 | Acc: 99.660% (879/882) 28/28 \n",
            " [============================>.] | Loss: 0.229 | Acc: 91.919% (91/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.021 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.246 | Acc: 89.899% (89/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.023 | Acc: 99.433% (877/882) 28/28 \n",
            " [============================>.] | Loss: 0.231 | Acc: 91.919% (91/99) 20/20 \n",
            "best_Test_acc: 91.919\n",
            "best_Test_acc_epoch: 23\n",
            "==> Preparing data..\n",
            "882 99\n",
            "882 99\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 2.228 | Acc: 26.984% (238/882) 28/28 \n",
            "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            " [============================>.] | Loss: 1.836 | Acc: 35.354% (35/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 35.354\n",
            "\n",
            "Epoch: 1\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.888 | Acc: 37.868% (334/882) 28/28 \n",
            " [============================>.] | Loss: 1.682 | Acc: 36.364% (36/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 36.364\n",
            "\n",
            "Epoch: 2\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.717 | Acc: 41.610% (367/882) 28/28 \n",
            " [============================>.] | Loss: 4.188 | Acc: 24.242% (24/99) 20/20 \n",
            "\n",
            "Epoch: 3\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.561 | Acc: 46.599% (411/882) 28/28 \n",
            " [============================>.] | Loss: 1.669 | Acc: 37.374% (37/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 37.374\n",
            "\n",
            "Epoch: 4\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.363 | Acc: 53.628% (473/882) 28/28 \n",
            " [============================>.] | Loss: 1.310 | Acc: 54.545% (54/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 54.545\n",
            "\n",
            "Epoch: 5\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.298 | Acc: 58.163% (513/882) 28/28 \n",
            " [============================>.] | Loss: 1.309 | Acc: 59.596% (59/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 59.596\n",
            "\n",
            "Epoch: 6\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.326 | Acc: 54.649% (482/882) 28/28 \n",
            " [============================>.] | Loss: 4.915 | Acc: 36.364% (36/99) 20/20 \n",
            "\n",
            "Epoch: 7\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.198 | Acc: 58.050% (512/882) 28/28 \n",
            " [============================>.] | Loss: 0.999 | Acc: 63.636% (63/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 63.636\n",
            "\n",
            "Epoch: 8\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.089 | Acc: 60.884% (537/882) 28/28 \n",
            " [============================>.] | Loss: 0.966 | Acc: 66.667% (66/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 66.667\n",
            "\n",
            "Epoch: 9\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.966 | Acc: 63.719% (562/882) 28/28 \n",
            " [============================>.] | Loss: 0.927 | Acc: 68.687% (68/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 68.687\n",
            "\n",
            "Epoch: 10\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.859 | Acc: 68.027% (600/882) 28/28 \n",
            " [============================>.] | Loss: 0.846 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 11\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.981 | Acc: 65.646% (579/882) 28/28 \n",
            " [============================>.] | Loss: 1.072 | Acc: 65.657% (65/99) 20/20 \n",
            "\n",
            "Epoch: 12\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 1.008 | Acc: 63.492% (560/882) 28/28 \n",
            " [============================>.] | Loss: 1.199 | Acc: 63.636% (63/99) 20/20 \n",
            "\n",
            "Epoch: 13\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.880 | Acc: 66.327% (585/882) 28/28 \n",
            " [============================>.] | Loss: 0.843 | Acc: 68.687% (68/99) 20/20 \n",
            "\n",
            "Epoch: 14\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.856 | Acc: 66.893% (590/882) 28/28 \n",
            " [============================>.] | Loss: 0.886 | Acc: 72.727% (72/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 72.727\n",
            "\n",
            "Epoch: 15\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.764 | Acc: 70.748% (624/882) 28/28 \n",
            " [============================>.] | Loss: 0.957 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 16\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.705 | Acc: 72.109% (636/882) 28/28 \n",
            " [============================>.] | Loss: 0.757 | Acc: 69.697% (69/99) 20/20 \n",
            "\n",
            "Epoch: 17\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.607 | Acc: 77.324% (682/882) 28/28 \n",
            " [============================>.] | Loss: 0.835 | Acc: 66.667% (66/99) 20/20 \n",
            "\n",
            "Epoch: 18\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.511 | Acc: 80.385% (709/882) 28/28 \n",
            " [============================>.] | Loss: 0.545 | Acc: 76.768% (76/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 76.768\n",
            "\n",
            "Epoch: 19\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.539 | Acc: 79.138% (698/882) 28/28 \n",
            " [============================>.] | Loss: 0.805 | Acc: 77.778% (77/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 77.778\n",
            "\n",
            "Epoch: 20\n",
            "learning_rate: 0.01\n",
            " [============================>.] | Loss: 0.423 | Acc: 83.560% (737/882) 28/28 \n",
            " [============================>.] | Loss: 0.588 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 21\n",
            "learning_rate: 0.008\n",
            " [============================>.] | Loss: 0.458 | Acc: 82.766% (730/882) 28/28 \n",
            " [============================>.] | Loss: 0.987 | Acc: 60.606% (60/99) 20/20 \n",
            "\n",
            "Epoch: 22\n",
            "learning_rate: 0.006400000000000001\n",
            " [============================>.] | Loss: 0.667 | Acc: 77.664% (685/882) 28/28 \n",
            " [============================>.] | Loss: 0.550 | Acc: 79.798% (79/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 79.798\n",
            "\n",
            "Epoch: 23\n",
            "learning_rate: 0.005120000000000001\n",
            " [============================>.] | Loss: 0.367 | Acc: 84.240% (743/882) 28/28 \n",
            " [============================>.] | Loss: 0.564 | Acc: 81.818% (81/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 81.818\n",
            "\n",
            "Epoch: 24\n",
            "learning_rate: 0.004096000000000001\n",
            " [============================>.] | Loss: 0.285 | Acc: 86.848% (766/882) 28/28 \n",
            " [============================>.] | Loss: 0.523 | Acc: 79.798% (79/99) 20/20 \n",
            "\n",
            "Epoch: 25\n",
            "learning_rate: 0.0032768000000000007\n",
            " [============================>.] | Loss: 0.283 | Acc: 87.982% (776/882) 28/28 \n",
            " [============================>.] | Loss: 0.387 | Acc: 83.838% (83/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 83.838\n",
            "\n",
            "Epoch: 26\n",
            "learning_rate: 0.002621440000000001\n",
            " [============================>.] | Loss: 0.217 | Acc: 90.930% (802/882) 28/28 \n",
            " [============================>.] | Loss: 0.486 | Acc: 84.848% (84/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 84.848\n",
            "\n",
            "Epoch: 27\n",
            "learning_rate: 0.002097152000000001\n",
            " [============================>.] | Loss: 0.228 | Acc: 90.703% (800/882) 28/28 \n",
            " [============================>.] | Loss: 0.543 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 28\n",
            "learning_rate: 0.001677721600000001\n",
            " [============================>.] | Loss: 0.205 | Acc: 91.497% (807/882) 28/28 \n",
            " [============================>.] | Loss: 0.616 | Acc: 78.788% (78/99) 20/20 \n",
            "\n",
            "Epoch: 29\n",
            "learning_rate: 0.0013421772800000006\n",
            " [============================>.] | Loss: 0.233 | Acc: 91.270% (805/882) 28/28 \n",
            " [============================>.] | Loss: 0.537 | Acc: 73.737% (73/99) 20/20 \n",
            "\n",
            "Epoch: 30\n",
            "learning_rate: 0.0010737418240000006\n",
            " [============================>.] | Loss: 0.176 | Acc: 93.197% (822/882) 28/28 \n",
            " [============================>.] | Loss: 0.483 | Acc: 80.808% (80/99) 20/20 \n",
            "\n",
            "Epoch: 31\n",
            "learning_rate: 0.0008589934592000006\n",
            " [============================>.] | Loss: 0.139 | Acc: 95.011% (838/882) 28/28 \n",
            " [============================>.] | Loss: 0.540 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 32\n",
            "learning_rate: 0.0006871947673600004\n",
            " [============================>.] | Loss: 0.192 | Acc: 93.084% (821/882) 28/28 \n",
            " [============================>.] | Loss: 0.468 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 33\n",
            "learning_rate: 0.0005497558138880004\n",
            " [============================>.] | Loss: 0.158 | Acc: 93.991% (829/882) 28/28 \n",
            " [============================>.] | Loss: 0.541 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 34\n",
            "learning_rate: 0.00043980465111040037\n",
            " [============================>.] | Loss: 0.152 | Acc: 94.671% (835/882) 28/28 \n",
            " [============================>.] | Loss: 0.522 | Acc: 82.828% (82/99) 20/20 \n",
            "\n",
            "Epoch: 35\n",
            "learning_rate: 0.0003518437208883203\n",
            " [============================>.] | Loss: 0.132 | Acc: 94.898% (837/882) 28/28 \n",
            " [============================>.] | Loss: 0.483 | Acc: 85.859% (85/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 85.859\n",
            "\n",
            "Epoch: 36\n",
            "learning_rate: 0.00028147497671065624\n",
            " [============================>.] | Loss: 0.146 | Acc: 93.537% (825/882) 28/28 \n",
            " [============================>.] | Loss: 0.496 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 37\n",
            "learning_rate: 0.00022517998136852504\n",
            " [============================>.] | Loss: 0.148 | Acc: 93.991% (829/882) 28/28 \n",
            " [============================>.] | Loss: 0.474 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 38\n",
            "learning_rate: 0.00018014398509482002\n",
            " [============================>.] | Loss: 0.119 | Acc: 95.351% (841/882) 28/28 \n",
            " [============================>.] | Loss: 0.532 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 39\n",
            "learning_rate: 0.00014411518807585602\n",
            " [============================>.] | Loss: 0.139 | Acc: 95.125% (839/882) 28/28 \n",
            " [============================>.] | Loss: 0.495 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 40\n",
            "learning_rate: 0.00011529215046068484\n",
            " [============================>.] | Loss: 0.134 | Acc: 95.238% (840/882) 28/28 \n",
            " [============================>.] | Loss: 0.468 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 41\n",
            "learning_rate: 9.223372036854788e-05\n",
            " [============================>.] | Loss: 0.137 | Acc: 95.238% (840/882) 28/28 \n",
            " [============================>.] | Loss: 0.490 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 42\n",
            "learning_rate: 7.37869762948383e-05\n",
            " [============================>.] | Loss: 0.118 | Acc: 95.692% (844/882) 28/28 \n",
            " [============================>.] | Loss: 0.496 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 43\n",
            "learning_rate: 5.902958103587064e-05\n",
            " [============================>.] | Loss: 0.104 | Acc: 96.599% (852/882) 28/28 \n",
            " [============================>.] | Loss: 0.515 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 44\n",
            "learning_rate: 4.722366482869652e-05\n",
            " [============================>.] | Loss: 0.137 | Acc: 95.465% (842/882) 28/28 \n",
            " [============================>.] | Loss: 0.518 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 45\n",
            "learning_rate: 3.777893186295722e-05\n",
            " [============================>.] | Loss: 0.114 | Acc: 96.259% (849/882) 28/28 \n",
            " [============================>.] | Loss: 0.501 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 46\n",
            "learning_rate: 3.0223145490365776e-05\n",
            " [============================>.] | Loss: 0.142 | Acc: 94.898% (837/882) 28/28 \n",
            " [============================>.] | Loss: 0.484 | Acc: 86.869% (86/99) 20/20 \n",
            "Saving..\n",
            "best_Test_acc: 86.869\n",
            "\n",
            "Epoch: 47\n",
            "learning_rate: 2.417851639229262e-05\n",
            " [============================>.] | Loss: 0.158 | Acc: 94.331% (832/882) 28/28 \n",
            " [============================>.] | Loss: 0.482 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 48\n",
            "learning_rate: 1.9342813113834096e-05\n",
            " [============================>.] | Loss: 0.132 | Acc: 95.238% (840/882) 28/28 \n",
            " [============================>.] | Loss: 0.521 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 49\n",
            "learning_rate: 1.547425049106728e-05\n",
            " [============================>.] | Loss: 0.114 | Acc: 96.145% (848/882) 28/28 \n",
            " [============================>.] | Loss: 0.507 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 50\n",
            "learning_rate: 1.2379400392853824e-05\n",
            " [============================>.] | Loss: 0.146 | Acc: 94.785% (836/882) 28/28 \n",
            " [============================>.] | Loss: 0.493 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 51\n",
            "learning_rate: 9.903520314283058e-06\n",
            " [============================>.] | Loss: 0.140 | Acc: 94.444% (833/882) 28/28 \n",
            " [============================>.] | Loss: 0.495 | Acc: 83.838% (83/99) 20/20 \n",
            "\n",
            "Epoch: 52\n",
            "learning_rate: 7.922816251426448e-06\n",
            " [============================>.] | Loss: 0.154 | Acc: 95.011% (838/882) 28/28 \n",
            " [============================>.] | Loss: 0.510 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 53\n",
            "learning_rate: 6.338253001141158e-06\n",
            " [============================>.] | Loss: 0.151 | Acc: 93.991% (829/882) 28/28 \n",
            " [============================>.] | Loss: 0.470 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 54\n",
            "learning_rate: 5.0706024009129275e-06\n",
            " [============================>.] | Loss: 0.115 | Acc: 96.599% (852/882) 28/28 \n",
            " [============================>.] | Loss: 0.518 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 55\n",
            "learning_rate: 4.056481920730342e-06\n",
            " [============================>.] | Loss: 0.115 | Acc: 95.578% (843/882) 28/28 \n",
            " [============================>.] | Loss: 0.520 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 56\n",
            "learning_rate: 3.2451855365842735e-06\n",
            " [============================>.] | Loss: 0.133 | Acc: 94.671% (835/882) 28/28 \n",
            " [============================>.] | Loss: 0.494 | Acc: 85.859% (85/99) 20/20 \n",
            "\n",
            "Epoch: 57\n",
            "learning_rate: 2.5961484292674196e-06\n",
            " [============================>.] | Loss: 0.146 | Acc: 95.011% (838/882) 28/28 \n",
            " [============================>.] | Loss: 0.518 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 58\n",
            "learning_rate: 2.0769187434139356e-06\n",
            " [============================>.] | Loss: 0.093 | Acc: 97.052% (856/882) 28/28 \n",
            " [============================>.] | Loss: 0.526 | Acc: 84.848% (84/99) 20/20 \n",
            "\n",
            "Epoch: 59\n",
            "learning_rate: 1.6615349947311485e-06\n",
            " [============================>.] | Loss: 0.106 | Acc: 97.279% (858/882) 28/28 \n",
            " [============================>.] | Loss: 0.521 | Acc: 84.848% (84/99) 20/20 \n",
            "best_Test_acc: 86.869\n",
            "best_Test_acc_epoch: 46\n",
            "Train VGG19 ok!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwKiwUQdfX5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6ebcd11f-8d5c-4dd4-8e3e-a920242ed6ef"
      },
      "source": [
        "!python /content/Facial-Expression-Recognition.Pytorch/plot_fer2013_confusion_matrix.py"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Facial-Expression-Recognition.Pytorch/plot_fer2013_confusion_matrix.py\", line 82, in <module>\n",
            "    checkpoint = torch.load(os.path.join(path, opt.split + '_model.t7'))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 419, in load\n",
            "    f = open(f, 'rb')\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'FER2013_VGG19/PrivateTest_model.t7'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw7XUTjBv7SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}